---
title: 机器学习总结二
date: 2017-11-26 10:30
tags: [机器学习]
---

## 标签 ##

* [朴素贝叶斯](#算法三：朴素贝叶斯)
* [AdaBoost](#算法四：AdaBoost)
* [K—均值](#算法五：K—均值)

<!-- more -->

## 1. 算法三：朴素贝叶斯 ##

### 1.1. 特点 ###

- 优点：在数据较少的情况仍有效，可以处理多分类问题

- 缺点：对输入数据的准备方式较为敏感

- 适用数据范围：标称型（且特征条件独立）

### 1.2. 算法思想 ###

1. 记录训练集中各种类别的概率$P(C_i)$

2. 记录训练集中出现的各个特征的各种特征值的占全部的概率

3. 记录不同类别中各个特征的各种特征值在该类别的概率

4. 利用3中数据计算待测样品(x,y)在各类别中出现的概率$P(x,y|C_i)$；利用2中数据计算待测样品(x,y)出现的概率P(x,y)

5. 根据用贝叶斯准则计算出各个类别在待测样品中出现的概率$P(C_i|x,y)$

6. 比较$P(C_i|x,y)$，其中若概率值最大则预测待测样品为该类别(贝叶斯分类准则)

- 拉普拉斯平滑：在计算过程中，有可能出现概率值为0的情况。为避免这种情况，在一开始训练集记录前每个特征的每种特征值计数初始值为1。即在步骤2，3中几率过程分子和分母都分别加上一个常数。

### 1.3. 条件概率 ###

- 符号：
    
    P(A)：发生事件A的概率

    P(A|B)：在事件B发生下事件A的概率

    P(A,B)：即发生事件B又发生事件A的概率

- 贝叶斯准则：

    - 前提条件：各事件之间独立，则概率互不影响。

    $$ P(A|B) = \frac{P(B|A)P(A)}{P(B)} $$

    - 例子说明：
        
    一堆球有红色和绿色，又有大球和小球，其中球的大小和颜色无关
        
        P(红)：红球在全部中的概率；

        P(大)：大球在全部中的概率；

        P(红|大)：红球在大球中的概率；

        P(大|红)：大球在红球中的概率；

        P(红，大) ：即是大球又是红球在全部中的概率。

        $$ P(大|红) = \frac{P(红,大)}{P(红)} $$

        $$ P(红|大) = \frac{P(红,大)}{P(大)} $$

        整理上二式则得出：$$ P(大|红) = \frac{P(红|大)P(大)}{P(红)} $$

- 贝叶斯分类准则：

    数据(x,y),需要预测其属于类别$C_i$中的哪一类。则每类概率可通过贝叶斯准则算出：

    $$ P(C_i|x,y) = \frac{P(x,y|C_i)P(C_i)}{P(x,y)} $$
    
    若$P(C_k|x,y)$满足 $P(C_k|x,y) > P(C_m|x,y) (m ∈ i,m ≠ k)$ 那么数据(x,y)属于$C_k$这一类别




## 2. 算法四：AdaBoost ##

### 2.1. 特点 ###

- 优点：泛化错误率低，易编码，可以应用在大部分分类器上，无参数调整

- 缺点：对离群点敏感

- 适用数据范围：数值型和标称

### 2.2. boosting ###

#### 2.2.1. 自举汇聚法 ####

自举汇聚法(bootstrap aggregating)也称bagging方法，思想如下：

1. 从原始数据集选择S次得到S个新数据集的技术。每个数据集大小相等，都是通过在原始数据集中随机选择一个样本来进行替换而得到的。

2. 将某一算法作用在每个数据集中，得到S个分类器。

3. 用S个分类器分别预测测试数据的类别，取被预测最多的类别作为该数据的预测类别。

#### 2.2.2. boosting ####

boosting技术类似于bagging技术，但boosting中每个分类器和新数据集的产生不是并行的，每个分类器的权重也不尽相同。思想如下：

1. 为整个过程设置分类器的个数,并为训练集每个数据设置相同的权重值$D_i^j$(第i个数据在第j次分类器的数据中的权重，此时j = 0)

2. 应用某一算法对数据进行训练得出一个权重错误$D_\varepsilon$最小的分类器，记录错误率$\varepsilon$，并计算该分类器的权重$\alpha_j$

3. 特别关注分类器错分的数据，更改每个数据的对应权重值$D_i^{(j)}$(j为迭代的次数)，重复2，直至分类器个数等于最大个数或错误率为0

4. 将待测数据用每个分类器进行预判，然后将结果通过每个分类器的权重进行加权平均计算出最后的结果

不同的boosting算法取决于其更改训练集数据权重方法，以及分类器权重的计算方法的不同。

### 2.3. adaboosting ###

adaboosting是boosting的一种。

- 权重错误: $ D_\varepsilon = Sum(D\_i), i属于为训练集中被错分的样品的序号的集合$

- 分类器权重的计算方法

    - 错误率: $ \varepsilon = \frac{m\_错}{m\_总} $

    - 分类器权重: $ \alpha = \frac{1}{2}ln(\frac{1-\varepsilon}{\varepsilon}) $

- 训练集数据权重更改方法(第j+1次)

    - 被正确分类的样品:

    $$D_i^{(j+1)} = \frac{D_i^{(j)}e^{-\alpha_j}}{Sum(D^{(j)})}$$

    - 被错误分类的样品: 

    $$D_i^{(j+1)} = \frac{D_i^{(j)}e^{\alpha_j}}{Sum(D^{(j)})}$$




## 3. 算法五：K—均值 ##

### 3.1. 特点 ###

- 优点：容易实现

- 缺点：可能收敛到局部最小值，在大规模数据中收敛慢较慢

- 适用数据范围：数值型

- 类别：无监督学习

### 3.2. 算法思想 ###

1. 设置簇的数目，创建k个点作为起始质心（一般随机选择）

2. 训练集中每一个点都分别计算与k个质心的距离

3. 将数据点分配到距其最近的质心的簇

4. 对k个簇分别计算簇中所有的点的各个特征的均值作为新的质心，重复步骤2、3，直至质心不再改变

5. 得到并输出k个簇和各自簇质心

- 若k—均值收敛到局部最小值：先将具有最大误差平方和(SSE, 簇中每个各个特征到质心的距离的平方之和的总和)的簇划分为两个簇。接着合并其它簇中最近的两个质心，或者合并两个使得总的SSE增幅最小的质心

### 3.3. 二分k-均值 ###

- 方法1：将所有点看成一个簇，将其进行二均值聚类得到两个簇。选择当前SSE最大的簇进行二分，直到簇数目为k。

- 方法2：进将所有点看成一个簇，将其进行二均值聚类得到两个簇。选择当前2分后使得总的SSE最小的簇进行二分，直到簇数目为k。
