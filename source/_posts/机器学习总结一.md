---
title: 机器学习总结一
date: 2017-10-23 11:44
tags: [机器学习]
---

## 标签 ##

* [K-近邻算法](#k-近邻算法)
* [决策树](#决策树)

## K-近邻算法 ##

### 特点 ###

- 优点：精度高、对异常值不敏感、无数据输入假定

- 缺点：计算复杂度高、空间复杂度高

- 适用数据范围：数值型(连续)和标称型(离散)

### 算法思想 ###

1. 分别计算待判断标签的数据$y^i$与m个已知标签的数据之间的距离；

2. 按照距离的大小进行排序，在m中选取距离最小的K个点；

3. 确定前K个点所在类别的出现频率，返回频率最高的类别作为$y^i$的预测标签类别。

### 距离计算方法 ###

一般使用曼哈顿距离或欧式距离（计算距离前可先归一化数值；计算距离时可为每个特征设定权重）

- 曼哈顿距离： $d(x,y)=\sqrt{\sum\_{k=1}^{n}|x\_k - y\_k|}$

- 欧式距离： $d(x,y)=\sqrt{\sum\_{k=1}^{n}(x\_k - y\_k)^2}$

### 归一化特征值 ###

- 归一化至[0,1] $X = (X - min)/(max - min)$

- 归一化至[-1,1] $X = 2 * (X - min)/(max - min) - 1$

## 决策树 ##

### 特点 ###

- 优点：计算复杂度不高，对中间缺失值不敏感，可以处理不相关特征数字

- 缺点：可能会产生过度匹配问题

- 适用数据范围：数值型和标称型

### 算法思想 ###

1. 在一堆特征中选取出最佳分配特征(信息增益最高的特征)将样本分为k堆(k为最佳分配特征的该特征类别数)，并不再将该特征列入待分特征中，而该特征将作为一个分类特征节点。

2. 在k堆中的每一堆重复操作1,直至该堆样品全是一类别标签则将这堆作为该标签子叶，或者该堆样品已经遍历完所有划分数据集的特征，则采用多数表决法决定该子叶的分类（即返回出现次数最多的分类名称）

3. 决策过程需要从决策树的根节点开始，待测数据与决策树中的特征节点进行比较，并按照比较结果选择下一比较分支，直到叶子节点作为最终的决策结果。

### 信息增益 ###

#### 熵 ####

- 熵：信息的期待值。

- 符号：H

- 熵的计算：

    - 如果待分类样品可能划分在多个分类中，若$x_i$类的概率是$p(x_i)$，则$x_i$的信息定义为：

    $$ l(x\_i) = -\log\_{2}{p(x\_i)} $$

    - 熵，需要用到每个类别的信息

    $$ H = -\sum\_{i=1}^{n}{p(x\_i)\log\_{2}{p(x\_i)}} $$

    - 例子：若待分类事物可能划分在2个分类中，概率分别为0.4，0.6，则熵为：

    $$ H = -0.4\log\_{2}{0.4}-0.6\log\_{2}{0.6} $$

#### 信息增益 ####

- 符号：△H 

- 计算：
    
    - 一堆已知标签样品有已知概率的多个分类，可以计算出当前的熵$H_0$。若任意选取其中一个特征将其分类为k堆(k为该特征的类别数)，又可以计算出每小堆一堆的熵。(下式中$p(x_i)$为对应每小一堆占总数的比例)

    $$ △H = H\_0 - \sum_{i=1}^{k}{p(x\_i) H\_i} $$

    - 例子：已知当前的熵$H_0$，用其中一个特征将其分类为2堆，样品数分别占原来的0.4，0.6，熵分别为$H_1$，$H_2$。

    $$ △H = H_0 - 0.4H_1 - 0.6H_2 $$