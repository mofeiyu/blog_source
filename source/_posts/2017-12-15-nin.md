title: Network In Network
date: 2017-12-15 11:00
tags: [DNN]
categories: DNN
---

本文主要介绍一篇2014年论文Network In Network(NIN)的思想，[Network in Network](https://arxiv.org/abs/1312.4400)

<!-- more -->

# 1. Why #

线性卷积层：抽象、泛化能力较低，当有大量的滤波器时，网络结构复杂，参数空间过大。
全连接层：参数多且易过拟合。
传统的CNNs：由线性卷积层、池化层、全连接层堆叠起来。缺点:因网络参数规模过大，GPU显存不够用等而限制网络层数的增加，从而限制模型的泛化能力。

> Classic convolutional neuron networks consist of alternatively stacked convolutional layers and spatial pooling layers. The convolutional layers generate feature maps by linear convolutional filters followed by nonlinear activation functions (rectifier, sigmoid, tanh, etc.). 

# 2. What #

提出Network in Network：
1.多层感知卷积层(mlpconv)替代传统的线性卷积层，增强网络提取抽象特征和泛化的能力：
    多层感知卷积层在跨通道（cross channel,cross feature map ）情况下，等价于卷积层+1×1卷积层，所以此时mlpconv层也叫cccp层（cascaded cross channel parametric pooling）。
2.全局平均池化层(Global Average Pooling)代替全连接层，减少参数空间，便于加深网络和训练，有效防止过拟合:

# 3. How #

## 3.1. 多层感知卷积层(MLP Convolution Layers) ##

![Alt text](/img/dl/NIN/Figure1.jpg)
Figure 1: Comparison of linear convolution layer and mlpconv layer.

如图1左侧所示，一般线性卷积层由卷积层和非线性激活函数(如relu)组成，计算过程如下：

> $ f\_{i,j,k} = \max(w\_k^Tx\_{i,j},0)$

![Alt text](/img/dl/NIN/Figure2.jpg)
Figure 2: The overall structure of Network In Network.

如图1右侧所示，多层感知卷积层可以看成是每个卷积的局部感受野中还包含了一个微型的多层网络（即：将多个过滤器对每个同一局部计算的全部的结果，进行多层全连接计算，而当层全连接的个数影响数据的降维或升维），图2显示该论文中使用的结构，其中包含三个多层感知卷积层。多层感知卷积层计算过程如下：

> $ f\_{i,j,k\_1}^1 = \max({w\_k^1}^Tx\_{i,j}+b\_{k\_1} ,0)$
> $\.\.\.$
> $ f\_{i,j,k\_n}^n = \max({w\_k^1}^Tf\_{i,j}^{n-1}+b\_{k\_n} ,0)$

例子：
Input size： 32x32x128
Output size： 32x32x256
<pre>
    <table>
       <tr>
          <td></td>
          <td>过滤器</td>
          <td>通道数</td>
          <td>参数规模</td>
          <td>总规模</td>
       </tr>
       <tr>
          <td>线性卷积层</td>
          <td>3x3</td>
          <td>256</td>
          <td>3x3x128x256+256</td>
          <td>295k</td>
       </tr>
       <tr>
          <td rowspan="2">多层感知卷积层</td>
          <td>1x1</td>
          <td>64</td>
          <td>128x64 + 64</td>
          <td rowspan="2">156k</td>
       </tr>
       <tr>
          <td>3x3</td>
          <td>256</td>
          <td>3x3x64x256+256</td>
       </tr>
    </table>
</pre>

## 3.2. 全局平均池化层(Global Average Pooling) ##

该文提出采用全局平均池化层替代传统CNN中的全连接层，对每个特征图一整张图片（即，一个滤波器计算出来的全部数据，也称一个通道的全部数据）进行全局均值池化，这样每张特征图都可以得到一个输出。在上述操作中不需要任何参数，故参数网络结构规模变小，避免过拟合。传统CNN用于多分类最后一层多采用全连接加上softmax，全局平均池化层直接让每张特征图输出输出类的特征。这要求在做1000个分类任务的时候，最后一层的前一层输出的特征图个数（即通道数）为1000。

> In this paper, we propose another strategy called global average pooling to replace the traditional fully connected layers in CNN. The idea is to generate one feature map for each corresponding category of the classification task in the last mlpconv layer. Instead of adding fully connected layers on top of the feature maps, we take the average of each feature map, and the resulting vector is fed directly into the softmax layer. 

## 3.3. Network In Network Structure ##

> The overall structure of NIN is a stack of mlpconv layers, on top of which lie the global average pooling and the objective cost layer. Sub-sampling layers can be added in between the mlpconv 4 layers as in CNN and maxout networks. Figure 2 shows an NIN with three mlpconv layers. Within each mlpconv layer, there is a three-layer perceptron. The number of layers in both NIN and the micro networks is flexible and can be tuned for specific tasks. 